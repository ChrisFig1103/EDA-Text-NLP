{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dependencies**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Paths**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_HOME =  os.getcwd()\n",
    "\n",
    "CLEANED_NORMALIZED_CORPUS = join(MY_HOME, 'CORPUS', 'cleaned-normalized')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **POS Tagging**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                filename  total_words  unique_words  total_sentences   \n",
      "0  article01_cleaned.txt         1366           892                1  \\\n",
      "1  article02_cleaned.txt          734           544                1   \n",
      "2  article03_cleaned.txt          514           387                1   \n",
      "3  article04_cleaned.txt         1084           815                1   \n",
      "4  article05_cleaned.txt          461           340                1   \n",
      "5  article06_cleaned.txt         2145          1369                1   \n",
      "\n",
      "   lexical_richness  sentence_complexity  \n",
      "0          0.653001               1366.0  \n",
      "1          0.741144                734.0  \n",
      "2          0.752918                514.0  \n",
      "3          0.751845               1084.0  \n",
      "4          0.737527                461.0  \n",
      "5          0.638228               2145.0  \n"
     ]
    }
   ],
   "source": [
    "archivos = os.listdir(CLEANED_NORMALIZED_CORPUS)\n",
    "\n",
    "# Estructuras para almacenar las estad√≠sticas\n",
    "stats = {\n",
    "    'filename': [],\n",
    "    'total_words': [],\n",
    "    'unique_words': [],\n",
    "    'total_sentences': [],\n",
    "    'lexical_richness': [],\n",
    "    'sentence_complexity': []\n",
    "}\n",
    "\n",
    "for archivo in archivos:\n",
    "    with open(join(CLEANED_NORMALIZED_CORPUS, archivo), 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "        words = word_tokenize(text)\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "      \n",
    "        total_words = len(words)\n",
    "        unique_words = len(set(words))\n",
    "        total_sentences = len(sentences)\n",
    "        lexical_richness = unique_words / total_words if total_words != 0 else 0\n",
    "        sentence_complexity = total_words / total_sentences if total_sentences != 0 else 0\n",
    "        \n",
    "    \n",
    "        stats['filename'].append(archivo)\n",
    "        stats['total_words'].append(total_words)\n",
    "        stats['unique_words'].append(unique_words)\n",
    "        stats['total_sentences'].append(total_sentences)\n",
    "        stats['lexical_richness'].append(lexical_richness)\n",
    "        stats['sentence_complexity'].append(sentence_complexity)\n",
    "\n",
    "df = pd.DataFrame(stats)\n",
    "\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
